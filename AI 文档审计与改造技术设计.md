

# ---

**ğŸ“˜ DocuFix: AI å…¼å®¹æ€§å®¡è®¡ä¸å¢å¼ºå¥—ä»¶ (v0.1.0)**

Project Code: docufix-core  
å®šä½: The "Lighthouse" for AI Documentation.  
æ ¸å¿ƒä»·å€¼: è®©æ—§æ–‡æ¡£åœ¨ä¸è¿ç§»çš„æƒ…å†µä¸‹ï¼Œåªéœ€ä¸€æ¡å‘½ä»¤ï¼Œå°±èƒ½è¢« Cursor/Copilot/Claude å®Œç¾ç†è§£å¹¶æ“ä½œã€‚

## ---

**1\. ç³»ç»Ÿæ¶æ„è®¾è®¡ (Architecture)**

æˆ‘ä»¬é‡‡ç”¨ **"Pipeline" (ç®¡é“)** æ¶æ„ã€‚æ•°æ®æµå‘æ˜ç¡®ï¼Œæ˜“äºæ‰©å±•ã€‚

* **Input Layer (è¾“å…¥å±‚):** æ”¯æŒ URL (åœ¨çº¿æ–‡æ¡£) æˆ– Local Path (æœ¬åœ° Markdown/HTML æ–‡ä»¶)ã€‚  
* **Parsing Layer (è§£æå±‚):** é’ˆå¯¹ Docusaurus, Sphinx, MkDocs ç­‰ä¸»æµæ¡†æ¶çš„ç‰¹å®šè§£æå™¨ï¼ˆå»é™¤ Header/Footer å™ªéŸ³ï¼‰ã€‚  
* **Audit Engine (å®¡è®¡å¼•æ“ \- æ ¸å¿ƒå£å’):** æ¨¡æ‹Ÿ RAG æ£€ç´¢è¿‡ç¨‹ï¼Œè®¡ç®—â€œå¹»è§‰é£é™©åˆ†â€ã€‚  
* **Retrofit Layer (æ”¹é€ å±‚):** ç”Ÿæˆ llms.txt (è¢«åŠ¨ç´¢å¼•) å’Œ mcp.json (ä¸»åŠ¨ Agent åè®®)ã€‚  
* **Presentation Layer (è¡¨ç°å±‚):** CLI ç»ˆç«¯æŠ¥å‘Š \+ CI/CD é˜»æ–­æœºåˆ¶ã€‚

## ---

**2\. æ ¸å¿ƒæ¨¡å—è¯¦ç»†è®¾è®¡ (Detailed Modules)**

### **2.1 ç›®å½•ç»“æ„ (Project Structure)**

Bash

docufix/  
â”œâ”€â”€ src/  
â”‚   â”œâ”€â”€ audit/           \# å®¡è®¡æ ¸å¿ƒç®—æ³•  
â”‚   â”‚   â”œâ”€â”€ metrics.py   \# è¯„åˆ†ç»´åº¦ (Tokenå¯†åº¦, ä»£ç å®Œæ•´æ€§ç­‰)  
â”‚   â”‚   â””â”€â”€ scorer.py    \# æ€»åˆ†è®¡ç®—å™¨  
â”‚   â”œâ”€â”€ parser/          \# è§£æä¸åŒç±»å‹çš„æ–‡æ¡£  
â”‚   â”‚   â”œâ”€â”€ html\_parser.py  
â”‚   â”‚   â””â”€â”€ markdown\_parser.py  
â”‚   â”œâ”€â”€ generator/       \# ç”Ÿæˆæ–‡ä»¶  
â”‚   â”‚   â”œâ”€â”€ llms\_txt.py  
â”‚   â”‚   â””â”€â”€ mcp\_server.py \# \[å·®å¼‚åŒ–\] MCP åè®®ç”Ÿæˆå™¨  
â”‚   â”œâ”€â”€ cli.py           \# Typer å…¥å£  
â”‚   â””â”€â”€ main.py  
â”œâ”€â”€ tests/  
â”œâ”€â”€ pyproject.toml  
â””â”€â”€ README.md

### **2.2 æ ¸å¿ƒç®—æ³•ï¼šå®¡è®¡å¼•æ“ (src/audit/scorer.py)**

è¿™æ˜¯ä½ çš„**å•†ä¸šæœºå¯†**ã€‚æ™®é€šçš„å·¥å…·åªæå–æ–‡æœ¬ï¼Œä½ çš„å·¥å…·ä¼šâ€œæ¨¡æ‹Ÿ AI çš„å¤§è„‘â€ã€‚

**è¯„åˆ†ç»´åº¦ (The DocuFix Score 0-100):**

1. **Chunk Health (åˆ‡ç‰‡å¥åº·åº¦) \- 40åˆ†**  
   * *åŸç†:* RAG é€šå¸¸æŒ‰ 512 æˆ– 1024 Token åˆ‡åˆ†ã€‚  
   * *æ‰£åˆ†é¡¹:* å¦‚æœ 500 ä¸ª Token å†…æ²¡æœ‰å‡ºç°ä»»ä½• Markdown æ ‡é¢˜ (\#, \#\#)ï¼Œæ‰£ 10 åˆ†ã€‚è¿™æ„å‘³ç€ AI è¯»åˆ°è¿™é‡Œä¼šä¸¢å¤±ä¸Šä¸‹æ–‡ï¼ˆä¸çŸ¥é“è¿™æ®µè¯å±äºå“ªä¸ªç« èŠ‚ï¼‰ã€‚  
2. **Code Context (ä»£ç ä¸Šä¸‹æ–‡) \- 30åˆ†**  
   * *åŸç†:* å¼€å‘è€…æœ€çœ‹é‡ä»£ç ã€‚  
   * *æ‰£åˆ†é¡¹:* ä»£ç å— (\<pre\>) è¶…è¿‡ 10 è¡Œä½†æ²¡æœ‰æ³¨é‡Šï¼Ÿæ‰£åˆ†ã€‚ä»£ç å—é‡Œç¼ºå°‘ import è¯­å¥ï¼ˆå¯¼è‡´ AI æ— æ³•æ¨æ–­åº“åï¼‰ï¼Ÿä¸¥é‡æ‰£åˆ†ã€‚  
3. **Link Rot (é“¾æ¥è…çƒ‚) \- 20åˆ†**  
   * *åŸç†:* AI é¡ºç€é“¾æ¥çˆ¬å–ï¼Œé‡åˆ° 404 ä¼šä¸­æ–­æ¨ç†ã€‚  
   * *æ‰£åˆ†é¡¹:* æ¯ä¸€ä¸ªæ­»é“¾æ‰£ 2 åˆ†ã€‚  
4. **Metadata (å…ƒæ•°æ®) \- 10åˆ†**  
   * *æ‰£åˆ†é¡¹:* ç¼ºå°‘ description æˆ– keywords meta æ ‡ç­¾ã€‚

### **2.3 å·®å¼‚åŒ–åŠŸèƒ½ï¼šMCP Server ç”Ÿæˆå™¨ (src/generator/mcp\_server.py)**

è¿™æ˜¯ä½ æ‰“å‡»ç«äº‰å¯¹æ‰‹çš„æ­¦å™¨ã€‚æˆ‘ä»¬ä¸ä»…ç”Ÿæˆé™æ€æ–‡æœ¬ï¼Œè¿˜ç”Ÿæˆ **Model Context Protocol (MCP)** é…ç½®ï¼Œè®© Claude Desktop å¯ä»¥ç›´æ¥æŒ‚è½½è¿™ä¸ªæ–‡æ¡£ã€‚

* **è¾“å‡ºæ–‡ä»¶:** docufix-mcp.json  
* **å†…å®¹é€»è¾‘:** è‡ªåŠ¨å°†æ–‡æ¡£çš„ OpenAPI (Swagger) å®šä¹‰è½¬æ¢ä¸º MCP Tools æ ¼å¼ã€‚

## ---

**3\. å…³é”®ä»£ç å®æ–½è§„èŒƒ (Implementation Specs)**

### **3.1 å®¡è®¡ç®—æ³•ä¼ªä»£ç  (Audit Logic)**

Python

\# src/audit/metrics.py  
import tiktoken  
from bs4 import BeautifulSoup

class ContextAuditor:  
    def \_\_init\_\_(self):  
        self.encoder \= tiktoken.get\_encoding("cl100k\_base")  
        self.chunk\_size \= 512

    def audit\_chunking\_risk(self, text: str) \-\> list\[dict\]:  
        """  
        æ£€æµ‹æ˜¯å¦æœ‰é•¿æ–‡æœ¬ç¼ºä¹ç»“æ„åŒ–æ ‡é¢˜ï¼Œå¯¼è‡´ RAG ä¸¢å¤±ä¸Šä¸‹æ–‡ã€‚  
        """  
        tokens \= self.encoder.encode(text)  
        chunks \= \[tokens\[i:i+self.chunk\_size\] for i in range(0, len(tokens), self.chunk\_size)\]  
          
        risks \= \[\]  
        for idx, chunk in enumerate(chunks):  
            chunk\_text \= self.encoder.decode(chunk)  
            \# å¦‚æœè¿™ä¸€å—é‡Œæ²¡æœ‰æ ‡é¢˜ (markdown header)ï¼Œè§†ä¸ºé«˜é£é™©  
            if "\# " not in chunk\_text and "\#\# " not in chunk\_text:  
                risks.append({  
                    "chunk\_id": idx,  
                    "risk\_level": "HIGH",  
                    "reason": "Context Drift: No headers found in 512 token window."  
                })  
        return risks

    def audit\_code\_blocks(self, html\_content: str) \-\> int:  
        """  
        æ£€æµ‹ä»£ç å—æ˜¯å¦åŒ…å«å¿…è¦çš„ import è¯­å¥  
        """  
        soup \= BeautifulSoup(html\_content, 'html.parser')  
        score \= 100  
        for code in soup.find\_all('code'):  
            text \= code.get\_text()  
            if len(text.split('\\n')) \> 5: \# è¿™æ˜¯ä¸€ä¸ªé•¿ä»£ç å—  
                \# ç®€å•å¯å‘å¼ï¼šPython ä»£ç åº”è¯¥æœ‰ import/from  
                if "import " not in text and "from " not in text:  
                    score \-= 5  
        return max(0, score)

### **3.2 CLI äº¤äº’è®¾è®¡ (Typer \+ Rich)**

ç”¨æˆ·ä½“éªŒå¿…é¡»åƒ npm audit ä¸€æ ·ä¸æ»‘ã€‚

Python

\# src/cli.py  
import typer  
from rich.console import Console  
from rich.table import Table

app \= typer.Typer()  
console \= Console()

@app.command()  
def scan(url: str):  
    """  
    è¿è¡Œ AI å…¼å®¹æ€§å®¡è®¡  
    """  
    console.print(f"\[bold blue\]DocuFix\[/\] Scanning {url} for AI-readability...", style="blink")  
      
    \# ... è°ƒç”¨å®¡è®¡é€»è¾‘ ...  
    score \= 72  \# å‡è®¾å¾—åˆ†  
      
    \# æ‰“å°æ¼‚äº®çš„æŠ¥å‘Šè¡¨  
    table \= Table(title="Audit Report")  
    table.add\_column("Metric", style="cyan")  
    table.add\_column("Status", style="magenta")  
    table.add\_column("Score Impact", style="red")  
      
    table.add\_row("Chunking Structure", "âš ï¸ Poor Headers", "-15")  
    table.add\_row("Code Snippets", "âœ… Healthy", "0")  
      
    console.print(table)  
    console.print(f"\\n\[bold yellow\]Final GEO Score: {score}/100\[/\]")  
      
    if score \< 80:  
        console.print("\[red\]Critical Issues Found\! AI models will hallucinate on this doc.\[/red\]")  
        console.print("Run \[bold green\]docufix fix\[/\] to generate patches.")

## ---

**4\. å¼€å‘å†²åˆºè®¡åˆ’ (Development Sprint Plan)**

åˆ©ç”¨ Antigravity çš„ Agent èƒ½åŠ›ï¼Œæˆ‘ä»¬æŠŠå¼€å‘å‹ç¼©åˆ° 7 å¤©ã€‚

### **Day 1: éª¨æ¶ä¸çˆ¬è™« (Infrastructure)**

* **ä»»åŠ¡:** æ­å»ºé¡¹ç›®ï¼Œå®ç° URL \-\> Clean Text çš„è½¬æ¢ã€‚  
* **éš¾ç‚¹:** å¤„ç† Docusaurus ç­‰å•é¡µåº”ç”¨ (SPA) çš„åŠ¨æ€åŠ è½½ã€‚  
* **Antigravity Prompt:**"Create a robust scraper using playwright-python. It needs to visit a URL, wait for network idle, and extract the content within the \<article\> or \<main\> tags. Strip out \<nav\> and \<footer\>. Save the clean HTML to a temp file."

### **Day 2-3: å®¡è®¡å¤§è„‘ (The Brain)**

* **ä»»åŠ¡:** å®ç° src/audit/scorer.pyã€‚  
* **æ ¸å¿ƒ:** æŠŠä¸Šé¢æåˆ°çš„ ContextAuditor é€»è¾‘å†™æ­»ã€‚  
* **æµ‹è¯•:** æ‰¾ä¸€ä¸ªå¾ˆçƒ‚çš„æ–‡æ¡£ï¼ˆæ¯”å¦‚æ²¡æœ‰ä»»ä½•æ ‡é¢˜çš„ä¸€å¤§æ®µæ–‡å­—ï¼‰å’Œä¸€ä¸ªå¾ˆå¥½çš„æ–‡æ¡£ï¼Œç¡®ä¿åˆ†æ•°æœ‰æ˜¾è‘—å·®å¼‚ã€‚

### **Day 4: ç”Ÿæˆå™¨ (The Fix)**

* **ä»»åŠ¡:** ç®€å•çš„ llms.txt ç”Ÿæˆã€‚  
* **æ ¼å¼:** éµå¾ª [llmstxt.org](https://llmstxt.org/) æ ‡å‡†ã€‚

### **Day 5: æ€æ‰‹é” MCP (The Differentiator)**

* **ä»»åŠ¡:** ç¼–å†™ä¸€ä¸ªè½¬æ¢å™¨ï¼ŒæŠŠæ–‡æ¡£é‡Œçš„ API æè¿°ï¼ˆå¦‚æœèƒ½æŠ“å–åˆ°ï¼‰è½¬æˆç®€å•çš„ JSON æè¿°ã€‚  
* **MVP:** å“ªæ€•åªæ˜¯ç”Ÿæˆä¸€ä¸ª JSON æ–‡ä»¶ï¼Œå‘Šè¯‰ Claude "Visit this URL to search docs"ï¼Œä¹Ÿæ¯”æ™®é€šæ–‡æ¡£å¼ºã€‚

### **Day 6-7: åŒ…è£…ä¸å‘å¸ƒ (Launch)**

* **ä»»åŠ¡:** ç¼–å†™ README.mdï¼Œå½•åˆ¶ä¸€ä¸ª GIFï¼ˆå±•ç¤ºç»ˆç«¯é‡Œè·‘åˆ†çš„é…·ç‚«æ•ˆæœï¼‰ã€‚  
* **å‘å¸ƒ:** PyPI, GitHubã€‚

## ---

**5\. ç«‹å³æ‰§è¡Œï¼šä½ çš„ç¬¬ä¸€æ¡æŒ‡ä»¤**

**è¯·å¤åˆ¶ä¸‹é¢çš„æŒ‡ä»¤åˆ°ä½ çš„ Google Antigravity (æˆ– Cursor) çš„ Composer ä¸­ï¼Œå¼€å§‹æ„å»ºé¡¹ç›®éª¨æ¶ï¼š**

Plaintext

Act as a Senior Python Architect. I want to build a CLI tool called "DocuFix".  
Purpose: Audit documentation websites for "AI-readability" (GEO).

Please generate the project structure and the following key files:

1\. \`pyproject.toml\`: Include dependencies \`typer\`, \`rich\`, \`beautifulsoup4\`, \`playwright\`, \`tiktoken\`.  
2\. \`src/cli.py\`: A basic Typer app with a \`scan(url)\` command. Use \`rich\` to print a placeholder score table.  
3\. \`src/audit/scorer.py\`: Create a class \`DocAuditor\`. Include a method \`check\_token\_density(text)\` using \`tiktoken\` to count tokens.  
4\. \`src/parser/web.py\`: A function stub \`fetch\_clean\_content(url)\` that simulates fetching a webpage.

Follow modern Python practices (type hinting, docstrings). Do not implement the full logic yet, just the scaffold.

